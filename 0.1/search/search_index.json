{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "concepts/", "text": "", "title": "Concepts"}, {"location": "getting_started/", "text": "Installation \u00b6", "title": "Getting Started"}, {"location": "getting_started/#installation", "text": "", "title": "Installation"}, {"location": "tutorials/", "text": "", "title": "Tutorials"}, {"location": "about/changelog/", "text": "Changelog \u00b6 Observes Semantic Versioning standard and Keep a Changelog convention. [0.1.5] - 2022-07-11 \u00b6 Add - New QualityMetrics table to store clusters' and waveforms' metrics after the spike sorting analysis. 0.1.4 - 2022-07-11 \u00b6 Bugfix - Handle case where spike_depths data is present. 0.1.3 - 2022-06-16 \u00b6 Update - Allow for the precluster_output_dir attribute to be nullable when no pre-clustering is performed. 0.1.2 - 2022-06-09 \u00b6 Bugfix - Handle case where pc_features.npy does not exist. 0.1.1 - 2022-06-01 \u00b6 Add - Secondary attributes to PreClusterParamSteps table 0.1.0 - 2022-05-26 \u00b6 Update - Rename module for acute probe insertions from ephys.py to ephys_acute.py . Add - Module for pre-clustering steps ( ephys_precluster.py ), which is built off of ephys_acute.py . Add - Module for chronic probe insertions ( ephys_chronic.py ). Bugfix - Missing fileTimeSecs key in SpikeGLX meta file. Update - Move common functions to element-interface package. Add - NWB export function 0.1.0b0 - 2021-05-07 \u00b6 Update - First beta release 0.1.0a5 - 2021-05-05 \u00b6 Add - GitHub Action release process Add - probe and ephys elements Add - Readers for: SpikeGLX , Open Ephys , Kilosort Add - Probe table supporting: Neuropixels probes 1.0 - 3A, 1.0 - 3B, 2.0 - SS, 2.0 - MS", "title": "Changelog"}, {"location": "about/changelog/#changelog", "text": "Observes Semantic Versioning standard and Keep a Changelog convention.", "title": "Changelog"}, {"location": "about/changelog/#015-2022-07-11", "text": "Add - New QualityMetrics table to store clusters' and waveforms' metrics after the spike sorting analysis.", "title": "[0.1.5] - 2022-07-11"}, {"location": "about/changelog/#014-2022-07-11", "text": "Bugfix - Handle case where spike_depths data is present.", "title": "0.1.4 - 2022-07-11"}, {"location": "about/changelog/#013-2022-06-16", "text": "Update - Allow for the precluster_output_dir attribute to be nullable when no pre-clustering is performed.", "title": "0.1.3 - 2022-06-16"}, {"location": "about/changelog/#012-2022-06-09", "text": "Bugfix - Handle case where pc_features.npy does not exist.", "title": "0.1.2 - 2022-06-09"}, {"location": "about/changelog/#011-2022-06-01", "text": "Add - Secondary attributes to PreClusterParamSteps table", "title": "0.1.1 - 2022-06-01"}, {"location": "about/changelog/#010-2022-05-26", "text": "Update - Rename module for acute probe insertions from ephys.py to ephys_acute.py . Add - Module for pre-clustering steps ( ephys_precluster.py ), which is built off of ephys_acute.py . Add - Module for chronic probe insertions ( ephys_chronic.py ). Bugfix - Missing fileTimeSecs key in SpikeGLX meta file. Update - Move common functions to element-interface package. Add - NWB export function", "title": "0.1.0 - 2022-05-26"}, {"location": "about/changelog/#010b0-2021-05-07", "text": "Update - First beta release", "title": "0.1.0b0 - 2021-05-07"}, {"location": "about/changelog/#010a5-2021-05-05", "text": "Add - GitHub Action release process Add - probe and ephys elements Add - Readers for: SpikeGLX , Open Ephys , Kilosort Add - Probe table supporting: Neuropixels probes 1.0 - 3A, 1.0 - 3B, 2.0 - SS, 2.0 - MS", "title": "0.1.0a5 - 2021-05-05"}, {"location": "api/element_array_ephys/__init__/", "text": "", "title": "__init__.py"}, {"location": "api/element_array_ephys/ephys_acute/", "text": "Clustering \u00b6 Bases: dj . Imported A processing table to handle each ClusteringTask: + If task_mode == \"trigger\" : trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_acute.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 @schema class Clustering ( dj . Imported ): \"\"\" A processing table to handle each ClusteringTask: + If `task_mode == \"trigger\"`: trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'load' : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' clustering analysis is not yet supported' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'clustering_time' : creation_time }) Curation \u00b6 Bases: dj . Manual Source code in element_array_ephys/ephys_acute.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 @schema class Curation ( dj . Manual ): definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note }) create1_from_clustering_task ( key , curation_note = '' ) \u00b6 A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_acute.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note }) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the ephys element :param probe_schema_name: schema name on the database server to activate the probe element - may be omitted if the probe element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the ephys element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_acute.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\" activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the `ephys` element :param probe_schema_name: schema name on the database server to activate the `probe` element - may be omitted if the `probe` element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the `ephys` element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \\ \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_acute.py 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 def generate_electrode_config ( probe_type : str , electrodes : list ): \"\"\" Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ 'electrode' ]: k for k in electrodes }) electrode_list = sorted ([ k [ 'electrode' ] for k in electrodes ]) electrode_gaps = ([ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ]) electrode_config_name = '; ' . join ([ f ' { electrode_list [ start + 1 ] } - { electrode_list [ end ] } ' for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :])]) electrode_config_key = { 'electrode_config_hash' : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ({ ** electrode_config_key , 'probe_type' : probe_type , 'electrode_config_name' : electrode_config_name }) probe . ElectrodeConfig . Electrode . insert ({ ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories Source code in element_array_ephys/ephys_acute.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def get_ephys_root_data_dir () -> list : \"\"\" All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories \"\"\" return _linking_module . get_ephys_root_data_dir () get_session_directory ( session_key ) \u00b6 get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for relative or full path to the session directory Source code in element_array_ephys/ephys_acute.py 78 79 80 81 82 83 84 85 86 def get_session_directory ( session_key : dict ) -> str : \"\"\" get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for relative or full path to the session directory \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "ephys_acute.py"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering", "text": "Bases: dj . Imported A processing table to handle each ClusteringTask: + If task_mode == \"trigger\" : trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_acute.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 @schema class Clustering ( dj . Imported ): \"\"\" A processing table to handle each ClusteringTask: + If `task_mode == \"trigger\"`: trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'load' : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' clustering analysis is not yet supported' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'clustering_time' : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation", "text": "Bases: dj . Manual Source code in element_array_ephys/ephys_acute.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 @schema class Curation ( dj . Manual ): definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note })", "title": "Curation"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation.create1_from_clustering_task", "text": "A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_acute.py 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note })", "title": "create1_from_clustering_task()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.activate", "text": "activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the ephys element :param probe_schema_name: schema name on the database server to activate the probe element - may be omitted if the probe element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the ephys element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_acute.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\" activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the `ephys` element :param probe_schema_name: schema name on the database server to activate the `probe` element - may be omitted if the `probe` element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the `ephys` element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \\ \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_acute.py 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 def generate_electrode_config ( probe_type : str , electrodes : list ): \"\"\" Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ 'electrode' ]: k for k in electrodes }) electrode_list = sorted ([ k [ 'electrode' ] for k in electrodes ]) electrode_gaps = ([ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ]) electrode_config_name = '; ' . join ([ f ' { electrode_list [ start + 1 ] } - { electrode_list [ end ] } ' for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :])]) electrode_config_key = { 'electrode_config_hash' : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ({ ** electrode_config_key , 'probe_type' : probe_type , 'electrode_config_name' : electrode_config_name }) probe . ElectrodeConfig . Electrode . insert ({ ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_ephys_root_data_dir", "text": "All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories Source code in element_array_ephys/ephys_acute.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def get_ephys_root_data_dir () -> list : \"\"\" All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories \"\"\" return _linking_module . get_ephys_root_data_dir ()", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_session_directory", "text": "get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for relative or full path to the session directory Source code in element_array_ephys/ephys_acute.py 78 79 80 81 82 83 84 85 86 def get_session_directory ( session_key : dict ) -> str : \"\"\" get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for relative or full path to the session directory \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/ephys_chronic/", "text": "Clustering \u00b6 Bases: dj . Imported A processing table to handle each ClusteringTask: + If task_mode == \"trigger\" : trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_chronic.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 @schema class Clustering ( dj . Imported ): \"\"\" A processing table to handle each ClusteringTask: + If `task_mode == \"trigger\"`: trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'load' : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' clustering analysis is not yet supported' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'clustering_time' : creation_time }) Curation \u00b6 Bases: dj . Manual Source code in element_array_ephys/ephys_chronic.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 @schema class Curation ( dj . Manual ): definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to clustering root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A convenient function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note }) create1_from_clustering_task ( key , curation_note = '' ) \u00b6 A convenient function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_chronic.py 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A convenient function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note }) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the ephys element :param probe_schema_name: schema name on the database server to activate the probe element - may be omitted if the probe element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the ephys element: Upstream tables: + Subject: table referenced by ProbeInsertion, typically identifying the animal undergoing a probe insertion + Session: table referenced by EphysRecording, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_chronic.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\" activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the `ephys` element :param probe_schema_name: schema name on the database server to activate the `probe` element - may be omitted if the `probe` element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the `ephys` element: Upstream tables: + Subject: table referenced by ProbeInsertion, typically identifying the animal undergoing a probe insertion + Session: table referenced by EphysRecording, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \\ \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_chronic.py 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 def generate_electrode_config ( probe_type : str , electrodes : list ): \"\"\" Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ 'electrode' ]: k for k in electrodes }) electrode_list = sorted ([ k [ 'electrode' ] for k in electrodes ]) electrode_gaps = ([ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ]) electrode_config_name = '; ' . join ([ f ' { electrode_list [ start + 1 ] } - { electrode_list [ end ] } ' for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :])]) electrode_config_key = { 'electrode_config_hash' : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ({ ** electrode_config_key , 'probe_type' : probe_type , 'electrode_config_name' : electrode_config_name }) probe . ElectrodeConfig . Electrode . insert ({ ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories Source code in element_array_ephys/ephys_chronic.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def get_ephys_root_data_dir () -> list : \"\"\" All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories \"\"\" return _linking_module . get_ephys_root_data_dir () get_session_directory ( session_key ) \u00b6 get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_chronic.py 78 79 80 81 82 83 84 85 86 def get_session_directory ( session_key : dict ) -> str : \"\"\" get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "ephys_chronic.py"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering", "text": "Bases: dj . Imported A processing table to handle each ClusteringTask: + If task_mode == \"trigger\" : trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_chronic.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 @schema class Clustering ( dj . Imported ): \"\"\" A processing table to handle each ClusteringTask: + If `task_mode == \"trigger\"`: trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'load' : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' clustering analysis is not yet supported' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'clustering_time' : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation", "text": "Bases: dj . Manual Source code in element_array_ephys/ephys_chronic.py 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 @schema class Curation ( dj . Manual ): definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to clustering root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A convenient function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note })", "title": "Curation"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation.create1_from_clustering_task", "text": "A convenient function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_chronic.py 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A convenient function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note })", "title": "create1_from_clustering_task()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.activate", "text": "activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the ephys element :param probe_schema_name: schema name on the database server to activate the probe element - may be omitted if the probe element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the ephys element: Upstream tables: + Subject: table referenced by ProbeInsertion, typically identifying the animal undergoing a probe insertion + Session: table referenced by EphysRecording, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_chronic.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\" activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the `ephys` element :param probe_schema_name: schema name on the database server to activate the `probe` element - may be omitted if the `probe` element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the `ephys` element: Upstream tables: + Subject: table referenced by ProbeInsertion, typically identifying the animal undergoing a probe insertion + Session: table referenced by EphysRecording, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \\ \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_chronic.py 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 def generate_electrode_config ( probe_type : str , electrodes : list ): \"\"\" Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ 'electrode' ]: k for k in electrodes }) electrode_list = sorted ([ k [ 'electrode' ] for k in electrodes ]) electrode_gaps = ([ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ]) electrode_config_name = '; ' . join ([ f ' { electrode_list [ start + 1 ] } - { electrode_list [ end ] } ' for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :])]) electrode_config_key = { 'electrode_config_hash' : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ({ ** electrode_config_key , 'probe_type' : probe_type , 'electrode_config_name' : electrode_config_name }) probe . ElectrodeConfig . Electrode . insert ({ ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_ephys_root_data_dir", "text": "All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories Source code in element_array_ephys/ephys_chronic.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def get_ephys_root_data_dir () -> list : \"\"\" All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories \"\"\" return _linking_module . get_ephys_root_data_dir ()", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_session_directory", "text": "get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_chronic.py 78 79 80 81 82 83 84 85 86 def get_session_directory ( session_key : dict ) -> str : \"\"\" get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/ephys_precluster/", "text": "Clustering \u00b6 Bases: dj . Imported A processing table to handle each ClusteringTask: + If task_mode == \"trigger\" : trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_precluster.py 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 @schema class Clustering ( dj . Imported ): \"\"\" A processing table to handle each ClusteringTask: + If `task_mode == \"trigger\"`: trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'load' : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' clustering analysis is not yet supported' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'clustering_time' : creation_time }) Curation \u00b6 Bases: dj . Manual Source code in element_array_ephys/ephys_precluster.py 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 @schema class Curation ( dj . Manual ): definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note }) create1_from_clustering_task ( key , curation_note = '' ) \u00b6 A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_precluster.py 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note }) PreCluster \u00b6 Bases: dj . Imported A processing table to handle each PreClusterTask: + If task_mode == \"none\" : no pre-clustering performed + If task_mode == \"trigger\" : trigger pre-clustering analysis according to the PreClusterParamSet + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_precluster.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 @schema class PreCluster ( dj . Imported ): \"\"\" A processing table to handle each PreClusterTask: + If `task_mode == \"none\"`: no pre-clustering performed + If `task_mode == \"trigger\"`: trigger pre-clustering analysis according to the PreClusterParamSet + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" -> PreClusterTask --- precluster_time: datetime # time of generation of this set of pre-clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( PreClusterTask & key ) . fetch1 ( 'task_mode' , 'precluster_output_dir' ) precluster_output_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'none' : if len (( PreClusterParamSteps . Step & key ) . fetch ()) > 0 : raise ValueError ( 'There are entries in the PreClusterParamSteps.Step ' 'table and task_mode=none' ) creation_time = ( EphysRecording & key ) . fetch1 ( 'recording_datetime' ) elif task_mode == 'load' : acq_software = ( EphysRecording & key ) . fetch1 ( 'acq_software' ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( 'probe' ) if acq_software == 'SpikeGLX' : for meta_filepath in precluster_output_dir . rglob ( '*.ap.meta' ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : creation_time = spikeglx_meta . recording_time break else : raise FileNotFoundError ( 'No SpikeGLX data found for probe insertion: {} ' . format ( key )) else : raise NotImplementedError ( f 'Pre-clustering analysis of { acq_software } ' 'is not yet supported.' ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' pre-clustering analysis is not yet supported.' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'precluster_time' : creation_time }) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the ephys element :param probe_schema_name: schema name on the database server to activate the probe element - may be omitted if the probe element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the ephys element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_precluster.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\" activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the `ephys` element :param probe_schema_name: schema name on the database server to activate the `probe` element - may be omitted if the `probe` element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the `ephys` element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \\ \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_precluster.py 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 def generate_electrode_config ( probe_type : str , electrodes : list ): \"\"\" Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ 'electrode' ]: k for k in electrodes }) electrode_list = sorted ([ k [ 'electrode' ] for k in electrodes ]) electrode_gaps = ([ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ]) electrode_config_name = '; ' . join ([ f ' { electrode_list [ start + 1 ] } - { electrode_list [ end ] } ' for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :])]) electrode_config_key = { 'electrode_config_hash' : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ({ ** electrode_config_key , 'probe_type' : probe_type , 'electrode_config_name' : electrode_config_name }) probe . ElectrodeConfig . Electrode . insert ({ ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories Source code in element_array_ephys/ephys_precluster.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def get_ephys_root_data_dir () -> list : \"\"\" All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories \"\"\" return _linking_module . get_ephys_root_data_dir () get_session_directory ( session_key ) \u00b6 get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for relative or full path to the session directory Source code in element_array_ephys/ephys_precluster.py 78 79 80 81 82 83 84 85 86 def get_session_directory ( session_key : dict ) -> str : \"\"\" get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for relative or full path to the session directory \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "ephys_precluster.py"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering", "text": "Bases: dj . Imported A processing table to handle each ClusteringTask: + If task_mode == \"trigger\" : trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_precluster.py 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 @schema class Clustering ( dj . Imported ): \"\"\" A processing table to handle each ClusteringTask: + If `task_mode == \"trigger\"`: trigger clustering analysis according to the ClusteringParamSet (e.g. launch a kilosort job) + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'load' : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' clustering analysis is not yet supported' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'clustering_time' : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation", "text": "Bases: dj . Manual Source code in element_array_ephys/ephys_precluster.py 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 @schema class Curation ( dj . Manual ): definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note })", "title": "Curation"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation.create1_from_clustering_task", "text": "A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_precluster.py 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def create1_from_clustering_task ( self , key , curation_note = '' ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f 'No corresponding entry in Clustering available' f ' for: { key } ; do `Clustering.populate(key)`' ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( 'task_mode' , 'clustering_output_dir' ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = dj . U () . aggr ( self & key , n = 'ifnull(max(curation_id)+1,1)' ) . fetch1 ( 'n' ) self . insert1 ({ ** key , 'curation_id' : curation_id , 'curation_time' : creation_time , 'curation_output_dir' : output_dir , 'quality_control' : is_qc , 'manual_curation' : is_curated , 'curation_note' : curation_note })", "title": "create1_from_clustering_task()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster", "text": "Bases: dj . Imported A processing table to handle each PreClusterTask: + If task_mode == \"none\" : no pre-clustering performed + If task_mode == \"trigger\" : trigger pre-clustering analysis according to the PreClusterParamSet + If task_mode == \"load\" : verify output Source code in element_array_ephys/ephys_precluster.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 @schema class PreCluster ( dj . Imported ): \"\"\" A processing table to handle each PreClusterTask: + If `task_mode == \"none\"`: no pre-clustering performed + If `task_mode == \"trigger\"`: trigger pre-clustering analysis according to the PreClusterParamSet + If `task_mode == \"load\"`: verify output \"\"\" definition = \"\"\" -> PreClusterTask --- precluster_time: datetime # time of generation of this set of pre-clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): task_mode , output_dir = ( PreClusterTask & key ) . fetch1 ( 'task_mode' , 'precluster_output_dir' ) precluster_output_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == 'none' : if len (( PreClusterParamSteps . Step & key ) . fetch ()) > 0 : raise ValueError ( 'There are entries in the PreClusterParamSteps.Step ' 'table and task_mode=none' ) creation_time = ( EphysRecording & key ) . fetch1 ( 'recording_datetime' ) elif task_mode == 'load' : acq_software = ( EphysRecording & key ) . fetch1 ( 'acq_software' ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( 'probe' ) if acq_software == 'SpikeGLX' : for meta_filepath in precluster_output_dir . rglob ( '*.ap.meta' ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : creation_time = spikeglx_meta . recording_time break else : raise FileNotFoundError ( 'No SpikeGLX data found for probe insertion: {} ' . format ( key )) else : raise NotImplementedError ( f 'Pre-clustering analysis of { acq_software } ' 'is not yet supported.' ) elif task_mode == 'trigger' : raise NotImplementedError ( 'Automatic triggering of' ' pre-clustering analysis is not yet supported.' ) else : raise ValueError ( f 'Unknown task mode: { task_mode } ' ) self . insert1 ({ ** key , 'precluster_time' : creation_time })", "title": "PreCluster"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.activate", "text": "activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the ephys element :param probe_schema_name: schema name on the database server to activate the probe element - may be omitted if the probe element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the ephys element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for full path to the session directory Source code in element_array_ephys/ephys_precluster.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\" activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None) :param ephys_schema_name: schema name on the database server to activate the `ephys` element :param probe_schema_name: schema name on the database server to activate the `probe` element - may be omitted if the `probe` element is already activated :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. :param linking_module: a module name or a module containing the required dependencies to activate the `ephys` element: Upstream tables: + Session: parent table to ProbeInsertion, typically identifying a recording session + SkullReference: Reference table for InsertionLocation, specifying the skull reference used for probe insertion location (e.g. Bregma, Lambda) Functions: + get_ephys_root_data_dir() -> list Retrieve the root data directory - e.g. containing the raw ephys recording files for all subject/sessions. :return: a string for full path to the root data directory + get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for full path to the session directory \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \\ \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_precluster.py 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 def generate_electrode_config ( probe_type : str , electrodes : list ): \"\"\" Generate and insert new ElectrodeConfig :param probe_type: probe type (e.g. neuropixels 2.0 - SS) :param electrodes: list of the electrode dict (keys of the probe.ProbeType.Electrode table) :return: a dict representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ 'electrode' ]: k for k in electrodes }) electrode_list = sorted ([ k [ 'electrode' ] for k in electrodes ]) electrode_gaps = ([ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ]) electrode_config_name = '; ' . join ([ f ' { electrode_list [ start + 1 ] } - { electrode_list [ end ] } ' for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :])]) electrode_config_key = { 'electrode_config_hash' : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ({ ** electrode_config_key , 'probe_type' : probe_type , 'electrode_config_name' : electrode_config_name }) probe . ElectrodeConfig . Electrode . insert ({ ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_ephys_root_data_dir", "text": "All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories Source code in element_array_ephys/ephys_precluster.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def get_ephys_root_data_dir () -> list : \"\"\" All data paths, directories in DataJoint Elements are recommended to be stored as relative paths, with respect to some user-configured \"root\" directory, which varies from machine to machine (e.g. different mounted drive locations) get_ephys_root_data_dir() -> list This user-provided function retrieves the possible root data directories containing the ephys data for all subjects/sessions (e.g. acquired SpikeGLX or Open Ephys raw files, output files from spike sorting routines, etc.) :return: a string for full path to the ephys root data directory, or list of strings for possible root data directories \"\"\" return _linking_module . get_ephys_root_data_dir ()", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_session_directory", "text": "get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session key :return: a string for relative or full path to the session directory Source code in element_array_ephys/ephys_precluster.py 78 79 80 81 82 83 84 85 86 def get_session_directory ( session_key : dict ) -> str : \"\"\" get_session_directory(session_key: dict) -> str Retrieve the session directory containing the recorded Neuropixels data for a given Session :param session_key: a dictionary of one Session `key` :return: a string for relative or full path to the session directory \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/probe/", "text": "Neuropixels Probes ProbeType \u00b6 Bases: dj . Lookup Source code in element_array_ephys/probe.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @schema class ProbeType ( dj . Lookup ): definition = \"\"\" # Type of probe, with specific electrodes geometry defined probe_type: varchar(32) # e.g. neuropixels_1.0 \"\"\" class Electrode ( dj . Part ): definition = \"\"\" -> master electrode: int # electrode index, starts at 0 --- shank: int # shank index, starts at 0, advance left to right shank_col: int # column index, starts at 0, advance left to right shank_row: int # row index, starts at 0, advance tip to tail x_coord=NULL: float # (um) x coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe y_coord=NULL: float # (um) y coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe \"\"\" @staticmethod def create_neuropixels_probe ( probe_type = 'neuropixels 1.0 - 3A' ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: 1.0 (3A and 3B), 2.0 (SS and MS) For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" def build_electrodes ( site_count , col_spacing , row_spacing , white_spacing , col_count = 2 , shank_count = 1 , shank_spacing = 250 ): \"\"\" :param site_count: site count per shank :param col_spacing: (um) horrizontal spacing between sites :param row_spacing: (um) vertical spacing between columns :param white_spacing: (um) offset spacing :param col_count: number of column per shank :param shank_count: number of shank :param shank_spacing: spacing between shanks :return: \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ([ 0 , 0 + col_spacing ], row_count ) x_white_spaces = np . tile ([ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 )) x_coords = x_coords + x_white_spaces y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , 2 ) shank_cols = np . tile ([ 0 , 1 ], row_count ) shank_rows = np . repeat ( range ( row_count ), 2 ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ([{ 'electrode' : ( site_count * shank_no ) + e_id , 'shank' : shank_no , 'shank_col' : c_id , 'shank_row' : r_id , 'x_coord' : x + ( shank_no * shank_spacing ), 'y_coord' : y } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ))]) return npx_electrodes # ---- 1.0 3A ---- if probe_type == 'neuropixels 1.0 - 3A' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3A' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 1.0 3B ---- if probe_type == 'neuropixels 1.0 - 3B' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3B' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Single shank ---- if probe_type == 'neuropixels 2.0 - SS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - SS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Multi shank ---- if probe_type == 'neuropixels 2.0 - MS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - MS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) create_neuropixels_probe ( probe_type = 'neuropixels 1.0 - 3A' ) staticmethod \u00b6 Create ProbeType and Electrode for neuropixels probes: 1.0 (3A and 3B), 2.0 (SS and MS) For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing Source code in element_array_ephys/probe.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @staticmethod def create_neuropixels_probe ( probe_type = 'neuropixels 1.0 - 3A' ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: 1.0 (3A and 3B), 2.0 (SS and MS) For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" def build_electrodes ( site_count , col_spacing , row_spacing , white_spacing , col_count = 2 , shank_count = 1 , shank_spacing = 250 ): \"\"\" :param site_count: site count per shank :param col_spacing: (um) horrizontal spacing between sites :param row_spacing: (um) vertical spacing between columns :param white_spacing: (um) offset spacing :param col_count: number of column per shank :param shank_count: number of shank :param shank_spacing: spacing between shanks :return: \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ([ 0 , 0 + col_spacing ], row_count ) x_white_spaces = np . tile ([ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 )) x_coords = x_coords + x_white_spaces y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , 2 ) shank_cols = np . tile ([ 0 , 1 ], row_count ) shank_rows = np . repeat ( range ( row_count ), 2 ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ([{ 'electrode' : ( site_count * shank_no ) + e_id , 'shank' : shank_no , 'shank_col' : c_id , 'shank_row' : r_id , 'x_coord' : x + ( shank_no * shank_spacing ), 'y_coord' : y } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ))]) return npx_electrodes # ---- 1.0 3A ---- if probe_type == 'neuropixels 1.0 - 3A' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3A' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 1.0 3B ---- if probe_type == 'neuropixels 1.0 - 3B' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3B' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Single shank ---- if probe_type == 'neuropixels 2.0 - SS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - SS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Multi shank ---- if probe_type == 'neuropixels 2.0 - MS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - MS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) activate ( schema_name , * , create_schema = True , create_tables = True ) \u00b6 activate(schema_name, create_schema=True, create_tables=True) :param schema_name: schema name on the database server to activate the probe element :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. Source code in element_array_ephys/probe.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def activate ( schema_name , * , create_schema = True , create_tables = True ): \"\"\" activate(schema_name, create_schema=True, create_tables=True) :param schema_name: schema name on the database server to activate the `probe` element :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. \"\"\" schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables ) # Add neuropixels probes for probe_type in ( 'neuropixels 1.0 - 3A' , 'neuropixels 1.0 - 3B' , 'neuropixels 2.0 - SS' , 'neuropixels 2.0 - MS' ): ProbeType . create_neuropixels_probe ( probe_type )", "title": "probe.py"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType", "text": "Bases: dj . Lookup Source code in element_array_ephys/probe.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @schema class ProbeType ( dj . Lookup ): definition = \"\"\" # Type of probe, with specific electrodes geometry defined probe_type: varchar(32) # e.g. neuropixels_1.0 \"\"\" class Electrode ( dj . Part ): definition = \"\"\" -> master electrode: int # electrode index, starts at 0 --- shank: int # shank index, starts at 0, advance left to right shank_col: int # column index, starts at 0, advance left to right shank_row: int # row index, starts at 0, advance tip to tail x_coord=NULL: float # (um) x coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe y_coord=NULL: float # (um) y coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe \"\"\" @staticmethod def create_neuropixels_probe ( probe_type = 'neuropixels 1.0 - 3A' ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: 1.0 (3A and 3B), 2.0 (SS and MS) For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" def build_electrodes ( site_count , col_spacing , row_spacing , white_spacing , col_count = 2 , shank_count = 1 , shank_spacing = 250 ): \"\"\" :param site_count: site count per shank :param col_spacing: (um) horrizontal spacing between sites :param row_spacing: (um) vertical spacing between columns :param white_spacing: (um) offset spacing :param col_count: number of column per shank :param shank_count: number of shank :param shank_spacing: spacing between shanks :return: \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ([ 0 , 0 + col_spacing ], row_count ) x_white_spaces = np . tile ([ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 )) x_coords = x_coords + x_white_spaces y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , 2 ) shank_cols = np . tile ([ 0 , 1 ], row_count ) shank_rows = np . repeat ( range ( row_count ), 2 ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ([{ 'electrode' : ( site_count * shank_no ) + e_id , 'shank' : shank_no , 'shank_col' : c_id , 'shank_row' : r_id , 'x_coord' : x + ( shank_no * shank_spacing ), 'y_coord' : y } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ))]) return npx_electrodes # ---- 1.0 3A ---- if probe_type == 'neuropixels 1.0 - 3A' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3A' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 1.0 3B ---- if probe_type == 'neuropixels 1.0 - 3B' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3B' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Single shank ---- if probe_type == 'neuropixels 2.0 - SS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - SS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Multi shank ---- if probe_type == 'neuropixels 2.0 - MS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - MS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True )", "title": "ProbeType"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.create_neuropixels_probe", "text": "Create ProbeType and Electrode for neuropixels probes: 1.0 (3A and 3B), 2.0 (SS and MS) For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing Source code in element_array_ephys/probe.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @staticmethod def create_neuropixels_probe ( probe_type = 'neuropixels 1.0 - 3A' ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: 1.0 (3A and 3B), 2.0 (SS and MS) For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" def build_electrodes ( site_count , col_spacing , row_spacing , white_spacing , col_count = 2 , shank_count = 1 , shank_spacing = 250 ): \"\"\" :param site_count: site count per shank :param col_spacing: (um) horrizontal spacing between sites :param row_spacing: (um) vertical spacing between columns :param white_spacing: (um) offset spacing :param col_count: number of column per shank :param shank_count: number of shank :param shank_spacing: spacing between shanks :return: \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ([ 0 , 0 + col_spacing ], row_count ) x_white_spaces = np . tile ([ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 )) x_coords = x_coords + x_white_spaces y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , 2 ) shank_cols = np . tile ([ 0 , 1 ], row_count ) shank_rows = np . repeat ( range ( row_count ), 2 ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ([{ 'electrode' : ( site_count * shank_no ) + e_id , 'shank' : shank_no , 'shank_col' : c_id , 'shank_row' : r_id , 'x_coord' : x + ( shank_no * shank_spacing ), 'y_coord' : y } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ))]) return npx_electrodes # ---- 1.0 3A ---- if probe_type == 'neuropixels 1.0 - 3A' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3A' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 1.0 3B ---- if probe_type == 'neuropixels 1.0 - 3B' : electrodes = build_electrodes ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 ) probe_type = { 'probe_type' : 'neuropixels 1.0 - 3B' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Single shank ---- if probe_type == 'neuropixels 2.0 - SS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - SS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) # ---- 2.0 Multi shank ---- if probe_type == 'neuropixels 2.0 - MS' : electrodes = build_electrodes ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 ) probe_type = { 'probe_type' : 'neuropixels 2.0 - MS' } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ([{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True )", "title": "create_neuropixels_probe()"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.activate", "text": "activate(schema_name, create_schema=True, create_tables=True) :param schema_name: schema name on the database server to activate the probe element :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. Source code in element_array_ephys/probe.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def activate ( schema_name , * , create_schema = True , create_tables = True ): \"\"\" activate(schema_name, create_schema=True, create_tables=True) :param schema_name: schema name on the database server to activate the `probe` element :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. \"\"\" schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables ) # Add neuropixels probes for probe_type in ( 'neuropixels 1.0 - 3A' , 'neuropixels 1.0 - 3B' , 'neuropixels 2.0 - SS' , 'neuropixels 2.0 - MS' ): ProbeType . create_neuropixels_probe ( probe_type )", "title": "activate()"}, {"location": "api/element_array_ephys/version/", "text": "Package metadata.", "title": "version.py"}, {"location": "api/element_array_ephys/export/__init__/", "text": "", "title": "__init__.py"}, {"location": "api/element_array_ephys/export/nwb/__init__/", "text": "", "title": "__init__.py"}, {"location": "api/element_array_ephys/export/nwb/nwb/", "text": "LFPDataChunkIterator \u00b6 Bases: GenericDataChunkIterator DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) Source code in element_array_ephys/export/nwb/nwb.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class LFPDataChunkIterator ( GenericDataChunkIterator ): \"\"\" DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) \"\"\" def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 ( as_dict = True ) self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 )) def _get_data ( self , selection ): electrode = self . electrodes [ selection [ 1 ]][ 0 ] return ( self . lfp_electrodes_query & dict ( electrode = electrode )) . fetch1 ( \"lfp\" )[ selection [ 0 ], np . newaxis ] def _get_dtype ( self ): return self . _dtype def _get_maxshape ( self ): return self . n_tt , self . n_channels __init__ ( lfp_electrodes_query , chunk_length = 10000 ) \u00b6 Parameters \u00b6 lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode int, optional Chunks are blocks of disk space where data are stored contiguously and compressed Source code in element_array_ephys/export/nwb/nwb.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 ( as_dict = True ) self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 )) add_electrodes_to_nwb ( session_key , nwbfile ) \u00b6 Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"] Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile Source code in element_array_ephys/export/nwb/nwb.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def add_electrodes_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile \"\"\" electrodes_query = probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode for additional_attribute in [ \"shank_col\" , \"shank_row\" , \"shank\" ]: nwbfile . add_electrode_column ( name = electrodes_query . heading . attributes [ additional_attribute ] . name , description = electrodes_query . heading . attributes [ additional_attribute ] . comment , ) nwbfile . add_electrode_column ( name = \"id_in_probe\" , description = \"electrode id within the probe\" , ) for this_probe in ( ephys . ProbeInsertion * probe . Probe & session_key ) . fetch ( as_dict = True ): insertion_record = ( ephys . InsertionLocation & this_probe ) . fetch ( as_dict = True ) if len ( insertion_record ) == 1 : insert_location = json . dumps ( { k : v for k , v in insertion_record [ 0 ] . items () if k not in ephys . InsertionLocation . primary_key }, cls = DecimalEncoder , ) elif len ( insertion_record ) == 0 : insert_location = \"unknown\" else : raise DataJointError ( f 'Found multiple insertion locations for { this_probe } ' ) device = nwbfile . create_device ( name = this_probe [ \"probe\" ], description = this_probe . get ( \"probe_comment\" , None ), manufacturer = this_probe . get ( \"probe_type\" , None ), ) shank_ids = set (( probe . ProbeType . Electrode & this_probe ) . fetch ( \"shank\" )) for shank_id in shank_ids : electrode_group = nwbfile . create_electrode_group ( name = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , description = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , location = insert_location , device = device , ) electrodes_query = ( probe . ProbeType . Electrode & this_probe & dict ( shank = shank_id ) ) . fetch ( as_dict = True ) for electrode in electrodes_query : nwbfile . add_electrode ( group = electrode_group , filtering = \"unknown\" , imp =- 1.0 , x = np . nan , y = np . nan , z = np . nan , rel_x = electrode [ \"x_coord\" ], rel_y = electrode [ \"y_coord\" ], rel_z = np . nan , shank_col = electrode [ \"shank_col\" ], shank_row = electrode [ \"shank_row\" ], location = \"unknown\" , id_in_probe = electrode [ \"electrode\" ], shank = electrode [ \"shank\" ], ) add_ephys_lfp_from_dj_to_nwb ( session_key , nwbfile ) \u00b6 Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].timestamps Parameters \u00b6 session_key: dict nwbfile: NWBFile Source code in element_array_ephys/export/nwb/nwb.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def add_ephys_lfp_from_dj_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].timestamps Parameters ---------- session_key: dict nwbfile: NWBFile \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) nwb_lfp = pynwb . ecephys . LFP ( name = \"LFP\" ) ecephys_module . add ( nwb_lfp ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for lfp_record in ( ephys . LFP & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion & lfp_record ) . fetch1 ( \"probe\" ) lfp_electrodes_query = ephys . LFP . Electrode & lfp_record lfp_data = LFPDataChunkIterator ( lfp_electrodes_query ) nwb_lfp . create_electrical_series ( name = f \"ElectricalSeries { lfp_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = H5DataIO ( lfp_data , compression = True ), timestamps = lfp_record [ \"lfp_time_stamps\" ], electrodes = nwbfile . create_electrode_table_region ( name = \"electrodes\" , description = \"electrodes used for LFP\" , region = [ mapping [( probe_id , x )] for x in lfp_electrodes_query . fetch ( \"electrode\" ) ], ), ) add_ephys_lfp_from_source_to_nwb ( session_key , ephys_root_data_dir , nwbfile , end_frame = None ) \u00b6 Read the LFP data directly from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile int, optional use for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 def add_ephys_lfp_from_source_to_nwb ( session_key : dict , ephys_root_data_dir , nwbfile : pynwb . NWBFile , end_frame = None ): \"\"\" Read the LFP data directly from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile end_frame: int, optional use for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) lfp = pynwb . ecephys . LFP () ecephys_module . add ( lfp ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.lf\" ) else : raise ValueError ( f \"unsupported acq_software type: { ephys_recording_record [ 'acq_software' ] } \" ) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) lfp . add_electrical_series ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = extractor . get_sampling_frequency (), starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) ) add_ephys_recording_to_nwb ( session_key , ephys_root_data_dir , nwbfile , end_frame = None ) \u00b6 Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"] Parameters \u00b6 session_key: dict ephys_root_data_dir: str nwbfile: NWBFile int, optional Used for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def add_ephys_recording_to_nwb ( session_key : dict , ephys_root_data_dir : str , nwbfile : pynwb . NWBFile , end_frame : int = None , ): \"\"\" Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"] Parameters ---------- session_key: dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional Used for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.ap\" ) elif ephys_recording_record [ \"acq_software\" ] == \"OpenEphys\" : extractor = extractors . read_openephys ( file_path , stream_id = \"0\" ) else : raise ValueError ( f \"unsupported acq_software type: { ephys_recording_record [ 'acq_software' ] } \" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) nwbfile . add_acquisition ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = str ( ephys_recording_record ), data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = ephys_recording_record [ \"sampling_rate\" ], starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) ) add_ephys_units_to_nwb ( session_key , nwbfile , primary_clustering_paramset_idx = 0 ) \u00b6 Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use primary_clustering_paramset_idx to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional Source code in element_array_ephys/export/nwb/nwb.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def add_ephys_units_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile , primary_clustering_paramset_idx : int = 0 ): \"\"\" Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use `primary_clustering_paramset_idx` to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional \"\"\" if not ephys . ClusteringTask & session_key : warnings . warn ( f 'No unit data exists for session: { session_key } ' ) return if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) for paramset_record in ( ephys . ClusteringParamSet & ephys . CuratedClustering & session_key ) . fetch ( \"paramset_idx\" , \"clustering_method\" , \"paramset_desc\" , as_dict = True ): if paramset_record [ \"paramset_idx\" ] == primary_clustering_paramset_idx : units_table = create_units_table ( session_key , nwbfile , paramset_record , desc = paramset_record [ \"paramset_desc\" ], ) nwbfile . units = units_table else : name = f \"units_ { paramset_record [ 'clustering_method' ] } \" units_table = create_units_table ( session_key , nwbfile , paramset_record , name = name , desc = paramset_record [ \"paramset_desc\" ], ) ecephys_module = get_module ( nwbfile , \"ecephys\" ) ecephys_module . add ( units_table ) create_units_table ( session_key , nwbfile , paramset_record , name = 'units' , desc = 'data on spiking units' ) \u00b6 ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile paramset_record: int str, optional default=\"units\" str, optional default=\"data on spiking units\" Source code in element_array_ephys/export/nwb/nwb.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def create_units_table ( session_key : dict , nwbfile : pynwb . NWBFile , paramset_record , name = \"units\" , desc = \"data on spiking units\" ): \"\"\" ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile paramset_record: int name: str, optional default=\"units\" desc: str, optional default=\"data on spiking units\" \"\"\" # electrode id mapping mapping = get_electrodes_mapping ( nwbfile . electrodes ) units_table = pynwb . misc . Units ( name = name , description = desc ) # add additional columns to the units table for additional_attribute in [ \"cluster_quality_label\" , \"spike_depths\" ]: # The `index` parameter indicates whether the column is a \"ragged array,\" i.e. # whether each row of this column is a vector with potentially different lengths # for each row. units_table . add_column ( name = additional_attribute , description = ephys . CuratedClustering . Unit . heading . attributes [ additional_attribute ] . comment , index = additional_attribute == \"spike_depths\" , ) clustering_query = ( ephys . EphysRecording * ephys . ClusteringTask & session_key & paramset_record ) for unit in tqdm ( ( ephys . CuratedClustering . Unit & clustering_query . proj ()) . fetch ( as_dict = True ), desc = f \"creating units table for paramset { paramset_record [ 'paramset_idx' ] } \" , ): probe_id , shank_num = ( ephys . ProbeInsertion * ephys . CuratedClustering . Unit * probe . ProbeType . Electrode & unit ) . fetch1 ( \"probe\" , \"shank\" ) waveform_mean = ( ephys . WaveformSet . PeakWaveform () & clustering_query & unit ) . fetch1 ( \"peak_electrode_waveform\" ) units_table . add_row ( id = unit [ \"unit\" ], electrodes = [ mapping [( probe_id , unit [ \"electrode\" ])]], electrode_group = nwbfile . electrode_groups [ f \"probe { probe_id } _shank { shank_num } \" ], cluster_quality_label = unit [ \"cluster_quality_label\" ], spike_times = unit [ \"spike_times\" ], spike_depths = unit [ \"spike_depths\" ], waveform_mean = waveform_mean , ) return units_table ecephys_session_to_nwb ( session_key , raw = True , spikes = True , lfp = 'source' , end_frame = None , lab_key = None , project_key = None , protocol_key = None , nwbfile_kwargs = None ) \u00b6 Main function for converting ephys data to NWB Parameters \u00b6 session_key: dict bool Whether to include the raw data from source. SpikeGLX and OpenEphys are supported bool Whether to include CuratedClustering lfp \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries used to look up optional additional metadata dict, optional If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the minimal data for instantiating an NWBFile object. If element-session is being used, this argument can optionally be used to add over overwrite NWBFile fields. Source code in element_array_ephys/export/nwb/nwb.py 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 def ecephys_session_to_nwb ( session_key , raw = True , spikes = True , lfp = \"source\" , end_frame = None , lab_key = None , project_key = None , protocol_key = None , nwbfile_kwargs = None , ): \"\"\" Main function for converting ephys data to NWB Parameters ---------- session_key: dict raw: bool Whether to include the raw data from source. SpikeGLX and OpenEphys are supported spikes: bool Whether to include CuratedClustering lfp: \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP end_frame: int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries used to look up optional additional metadata nwbfile_kwargs: dict, optional - If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the minimal data for instantiating an NWBFile object. - If element-session is being used, this argument can optionally be used to add over overwrite NWBFile fields. \"\"\" session_to_nwb = getattr ( ephys . _linking_module , 'session_to_nwb' , False ) if session_to_nwb : nwbfile = session_to_nwb ( session_key , lab_key = lab_key , project_key = project_key , protocol_key = protocol_key , additional_nwbfile_kwargs = nwbfile_kwargs , ) else : nwbfile = pynwb . NWBFile ( ** nwbfile_kwargs ) ephys_root_data_dir = ephys . get_ephys_root_data_dir () if raw : add_ephys_recording_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) if spikes : add_ephys_units_to_nwb ( session_key , nwbfile ) if lfp == \"dj\" : add_ephys_lfp_from_dj_to_nwb ( session_key , nwbfile ) if lfp == \"source\" : add_ephys_lfp_from_source_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) return nwbfile gains_helper ( gains ) \u00b6 This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the channel_conversion field in addition to the conversion field so that each channel can be converted to volts using its own individual gain. Parameters \u00b6 gains: np.ndarray Returns \u00b6 dict conversion : float channel_conversion : np.ndarray Source code in element_array_ephys/export/nwb/nwb.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def gains_helper ( gains ): \"\"\" This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the `channel_conversion` field in addition to the `conversion` field so that each channel can be converted to volts using its own individual gain. Parameters ---------- gains: np.ndarray Returns ------- dict conversion : float channel_conversion : np.ndarray \"\"\" if all ( x == 1 for x in gains ): return dict ( conversion = 1e-6 , channel_conversion = None ) if all ( x == gains [ 0 ] for x in gains ): return dict ( conversion = 1e-6 * gains [ 0 ], channel_conversion = None ) return dict ( conversion = 1e-6 , channel_conversion = gains ) get_electrodes_mapping ( electrodes ) \u00b6 Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries. Parameters \u00b6 electrodes: hdmf.common.table.DynamicTable Returns \u00b6 dict Source code in element_array_ephys/export/nwb/nwb.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def get_electrodes_mapping ( electrodes ): \"\"\" Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries. Parameters ---------- electrodes: hdmf.common.table.DynamicTable Returns ------- dict \"\"\" return { ( electrodes [ \"group\" ][ idx ] . device . name , electrodes [ \"id_in_probe\" ][ idx ],): idx for idx in range ( len ( electrodes )) } write_nwb ( nwbfile , fname , check_read = True ) \u00b6 Export NWBFile Parameters \u00b6 nwbfile: pynwb.NWBFile fname: str Absolute path including *.nwb extension. bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. Source code in element_array_ephys/export/nwb/nwb.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 def write_nwb ( nwbfile , fname , check_read = True ): \"\"\" Export NWBFile Parameters ---------- nwbfile: pynwb.NWBFile fname: str Absolute path including `*.nwb` extension. check_read: bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. \"\"\" with pynwb . NWBHDF5IO ( fname , \"w\" ) as io : io . write ( nwbfile ) if check_read : with pynwb . NWBHDF5IO ( fname , \"r\" ) as io : io . read ()", "title": "nwb.py"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator", "text": "Bases: GenericDataChunkIterator DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) Source code in element_array_ephys/export/nwb/nwb.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class LFPDataChunkIterator ( GenericDataChunkIterator ): \"\"\" DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) \"\"\" def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 ( as_dict = True ) self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 )) def _get_data ( self , selection ): electrode = self . electrodes [ selection [ 1 ]][ 0 ] return ( self . lfp_electrodes_query & dict ( electrode = electrode )) . fetch1 ( \"lfp\" )[ selection [ 0 ], np . newaxis ] def _get_dtype ( self ): return self . _dtype def _get_maxshape ( self ): return self . n_tt , self . n_channels", "title": "LFPDataChunkIterator"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__", "text": "", "title": "__init__()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__--parameters", "text": "lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode int, optional Chunks are blocks of disk space where data are stored contiguously and compressed Source code in element_array_ephys/export/nwb/nwb.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 ( as_dict = True ) self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 ))", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb", "text": "Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"]", "title": "add_electrodes_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile Source code in element_array_ephys/export/nwb/nwb.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def add_electrodes_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile \"\"\" electrodes_query = probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode for additional_attribute in [ \"shank_col\" , \"shank_row\" , \"shank\" ]: nwbfile . add_electrode_column ( name = electrodes_query . heading . attributes [ additional_attribute ] . name , description = electrodes_query . heading . attributes [ additional_attribute ] . comment , ) nwbfile . add_electrode_column ( name = \"id_in_probe\" , description = \"electrode id within the probe\" , ) for this_probe in ( ephys . ProbeInsertion * probe . Probe & session_key ) . fetch ( as_dict = True ): insertion_record = ( ephys . InsertionLocation & this_probe ) . fetch ( as_dict = True ) if len ( insertion_record ) == 1 : insert_location = json . dumps ( { k : v for k , v in insertion_record [ 0 ] . items () if k not in ephys . InsertionLocation . primary_key }, cls = DecimalEncoder , ) elif len ( insertion_record ) == 0 : insert_location = \"unknown\" else : raise DataJointError ( f 'Found multiple insertion locations for { this_probe } ' ) device = nwbfile . create_device ( name = this_probe [ \"probe\" ], description = this_probe . get ( \"probe_comment\" , None ), manufacturer = this_probe . get ( \"probe_type\" , None ), ) shank_ids = set (( probe . ProbeType . Electrode & this_probe ) . fetch ( \"shank\" )) for shank_id in shank_ids : electrode_group = nwbfile . create_electrode_group ( name = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , description = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , location = insert_location , device = device , ) electrodes_query = ( probe . ProbeType . Electrode & this_probe & dict ( shank = shank_id ) ) . fetch ( as_dict = True ) for electrode in electrodes_query : nwbfile . add_electrode ( group = electrode_group , filtering = \"unknown\" , imp =- 1.0 , x = np . nan , y = np . nan , z = np . nan , rel_x = electrode [ \"x_coord\" ], rel_y = electrode [ \"y_coord\" ], rel_z = np . nan , shank_col = electrode [ \"shank_col\" ], shank_row = electrode [ \"shank_row\" ], location = \"unknown\" , id_in_probe = electrode [ \"electrode\" ], shank = electrode [ \"shank\" ], )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb", "text": "Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].timestamps", "title": "add_ephys_lfp_from_dj_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb--parameters", "text": "session_key: dict nwbfile: NWBFile Source code in element_array_ephys/export/nwb/nwb.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def add_ephys_lfp_from_dj_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"].timestamps Parameters ---------- session_key: dict nwbfile: NWBFile \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) nwb_lfp = pynwb . ecephys . LFP ( name = \"LFP\" ) ecephys_module . add ( nwb_lfp ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for lfp_record in ( ephys . LFP & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion & lfp_record ) . fetch1 ( \"probe\" ) lfp_electrodes_query = ephys . LFP . Electrode & lfp_record lfp_data = LFPDataChunkIterator ( lfp_electrodes_query ) nwb_lfp . create_electrical_series ( name = f \"ElectricalSeries { lfp_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = H5DataIO ( lfp_data , compression = True ), timestamps = lfp_record [ \"lfp_time_stamps\" ], electrodes = nwbfile . create_electrode_table_region ( name = \"electrodes\" , description = \"electrodes used for LFP\" , region = [ mapping [( probe_id , x )] for x in lfp_electrodes_query . fetch ( \"electrode\" ) ], ), )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb", "text": "Read the LFP data directly from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition", "title": "add_ephys_lfp_from_source_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile int, optional use for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 def add_ephys_lfp_from_source_to_nwb ( session_key : dict , ephys_root_data_dir , nwbfile : pynwb . NWBFile , end_frame = None ): \"\"\" Read the LFP data directly from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile end_frame: int, optional use for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) lfp = pynwb . ecephys . LFP () ecephys_module . add ( lfp ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.lf\" ) else : raise ValueError ( f \"unsupported acq_software type: { ephys_recording_record [ 'acq_software' ] } \" ) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) lfp . add_electrical_series ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = extractor . get_sampling_frequency (), starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb", "text": "Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"]", "title": "add_ephys_recording_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb--parameters", "text": "session_key: dict ephys_root_data_dir: str nwbfile: NWBFile int, optional Used for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def add_ephys_recording_to_nwb ( session_key : dict , ephys_root_data_dir : str , nwbfile : pynwb . NWBFile , end_frame : int = None , ): \"\"\" Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"] Parameters ---------- session_key: dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional Used for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.ap\" ) elif ephys_recording_record [ \"acq_software\" ] == \"OpenEphys\" : extractor = extractors . read_openephys ( file_path , stream_id = \"0\" ) else : raise ValueError ( f \"unsupported acq_software type: { ephys_recording_record [ 'acq_software' ] } \" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) nwbfile . add_acquisition ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = str ( ephys_recording_record ), data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = ephys_recording_record [ \"sampling_rate\" ], starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb", "text": "Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use primary_clustering_paramset_idx to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"]", "title": "add_ephys_units_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional Source code in element_array_ephys/export/nwb/nwb.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def add_ephys_units_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile , primary_clustering_paramset_idx : int = 0 ): \"\"\" Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use `primary_clustering_paramset_idx` to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional \"\"\" if not ephys . ClusteringTask & session_key : warnings . warn ( f 'No unit data exists for session: { session_key } ' ) return if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) for paramset_record in ( ephys . ClusteringParamSet & ephys . CuratedClustering & session_key ) . fetch ( \"paramset_idx\" , \"clustering_method\" , \"paramset_desc\" , as_dict = True ): if paramset_record [ \"paramset_idx\" ] == primary_clustering_paramset_idx : units_table = create_units_table ( session_key , nwbfile , paramset_record , desc = paramset_record [ \"paramset_desc\" ], ) nwbfile . units = units_table else : name = f \"units_ { paramset_record [ 'clustering_method' ] } \" units_table = create_units_table ( session_key , nwbfile , paramset_record , name = name , desc = paramset_record [ \"paramset_desc\" ], ) ecephys_module = get_module ( nwbfile , \"ecephys\" ) ecephys_module . add ( units_table )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table", "text": "ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"]", "title": "create_units_table()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile paramset_record: int str, optional default=\"units\" str, optional default=\"data on spiking units\" Source code in element_array_ephys/export/nwb/nwb.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def create_units_table ( session_key : dict , nwbfile : pynwb . NWBFile , paramset_record , name = \"units\" , desc = \"data on spiking units\" ): \"\"\" ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile paramset_record: int name: str, optional default=\"units\" desc: str, optional default=\"data on spiking units\" \"\"\" # electrode id mapping mapping = get_electrodes_mapping ( nwbfile . electrodes ) units_table = pynwb . misc . Units ( name = name , description = desc ) # add additional columns to the units table for additional_attribute in [ \"cluster_quality_label\" , \"spike_depths\" ]: # The `index` parameter indicates whether the column is a \"ragged array,\" i.e. # whether each row of this column is a vector with potentially different lengths # for each row. units_table . add_column ( name = additional_attribute , description = ephys . CuratedClustering . Unit . heading . attributes [ additional_attribute ] . comment , index = additional_attribute == \"spike_depths\" , ) clustering_query = ( ephys . EphysRecording * ephys . ClusteringTask & session_key & paramset_record ) for unit in tqdm ( ( ephys . CuratedClustering . Unit & clustering_query . proj ()) . fetch ( as_dict = True ), desc = f \"creating units table for paramset { paramset_record [ 'paramset_idx' ] } \" , ): probe_id , shank_num = ( ephys . ProbeInsertion * ephys . CuratedClustering . Unit * probe . ProbeType . Electrode & unit ) . fetch1 ( \"probe\" , \"shank\" ) waveform_mean = ( ephys . WaveformSet . PeakWaveform () & clustering_query & unit ) . fetch1 ( \"peak_electrode_waveform\" ) units_table . add_row ( id = unit [ \"unit\" ], electrodes = [ mapping [( probe_id , unit [ \"electrode\" ])]], electrode_group = nwbfile . electrode_groups [ f \"probe { probe_id } _shank { shank_num } \" ], cluster_quality_label = unit [ \"cluster_quality_label\" ], spike_times = unit [ \"spike_times\" ], spike_depths = unit [ \"spike_depths\" ], waveform_mean = waveform_mean , ) return units_table", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb", "text": "Main function for converting ephys data to NWB", "title": "ecephys_session_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb--parameters", "text": "session_key: dict bool Whether to include the raw data from source. SpikeGLX and OpenEphys are supported bool Whether to include CuratedClustering lfp \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries used to look up optional additional metadata dict, optional If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the minimal data for instantiating an NWBFile object. If element-session is being used, this argument can optionally be used to add over overwrite NWBFile fields. Source code in element_array_ephys/export/nwb/nwb.py 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 def ecephys_session_to_nwb ( session_key , raw = True , spikes = True , lfp = \"source\" , end_frame = None , lab_key = None , project_key = None , protocol_key = None , nwbfile_kwargs = None , ): \"\"\" Main function for converting ephys data to NWB Parameters ---------- session_key: dict raw: bool Whether to include the raw data from source. SpikeGLX and OpenEphys are supported spikes: bool Whether to include CuratedClustering lfp: \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP end_frame: int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries used to look up optional additional metadata nwbfile_kwargs: dict, optional - If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the minimal data for instantiating an NWBFile object. - If element-session is being used, this argument can optionally be used to add over overwrite NWBFile fields. \"\"\" session_to_nwb = getattr ( ephys . _linking_module , 'session_to_nwb' , False ) if session_to_nwb : nwbfile = session_to_nwb ( session_key , lab_key = lab_key , project_key = project_key , protocol_key = protocol_key , additional_nwbfile_kwargs = nwbfile_kwargs , ) else : nwbfile = pynwb . NWBFile ( ** nwbfile_kwargs ) ephys_root_data_dir = ephys . get_ephys_root_data_dir () if raw : add_ephys_recording_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) if spikes : add_ephys_units_to_nwb ( session_key , nwbfile ) if lfp == \"dj\" : add_ephys_lfp_from_dj_to_nwb ( session_key , nwbfile ) if lfp == \"source\" : add_ephys_lfp_from_source_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) return nwbfile", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper", "text": "This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the channel_conversion field in addition to the conversion field so that each channel can be converted to volts using its own individual gain.", "title": "gains_helper()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper--parameters", "text": "gains: np.ndarray", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper--returns", "text": "dict conversion : float channel_conversion : np.ndarray Source code in element_array_ephys/export/nwb/nwb.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def gains_helper ( gains ): \"\"\" This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the `channel_conversion` field in addition to the `conversion` field so that each channel can be converted to volts using its own individual gain. Parameters ---------- gains: np.ndarray Returns ------- dict conversion : float channel_conversion : np.ndarray \"\"\" if all ( x == 1 for x in gains ): return dict ( conversion = 1e-6 , channel_conversion = None ) if all ( x == gains [ 0 ] for x in gains ): return dict ( conversion = 1e-6 * gains [ 0 ], channel_conversion = None ) return dict ( conversion = 1e-6 , channel_conversion = gains )", "title": "Returns"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping", "text": "Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries.", "title": "get_electrodes_mapping()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping--parameters", "text": "electrodes: hdmf.common.table.DynamicTable", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping--returns", "text": "dict Source code in element_array_ephys/export/nwb/nwb.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def get_electrodes_mapping ( electrodes ): \"\"\" Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries. Parameters ---------- electrodes: hdmf.common.table.DynamicTable Returns ------- dict \"\"\" return { ( electrodes [ \"group\" ][ idx ] . device . name , electrodes [ \"id_in_probe\" ][ idx ],): idx for idx in range ( len ( electrodes )) }", "title": "Returns"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb", "text": "Export NWBFile", "title": "write_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb--parameters", "text": "nwbfile: pynwb.NWBFile fname: str Absolute path including *.nwb extension. bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. Source code in element_array_ephys/export/nwb/nwb.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 def write_nwb ( nwbfile , fname , check_read = True ): \"\"\" Export NWBFile Parameters ---------- nwbfile: pynwb.NWBFile fname: str Absolute path including `*.nwb` extension. check_read: bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. \"\"\" with pynwb . NWBHDF5IO ( fname , \"w\" ) as io : io . write ( nwbfile ) if check_read : with pynwb . NWBHDF5IO ( fname , \"r\" ) as io : io . read ()", "title": "Parameters"}, {"location": "api/element_array_ephys/readers/__init__/", "text": "", "title": "__init__.py"}, {"location": "api/element_array_ephys/readers/kilosort/", "text": "Kilosort \u00b6 Source code in element_array_ephys/readers/kilosort.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class Kilosort : kilosort_files = [ 'params.py' , 'amplitudes.npy' , 'channel_map.npy' , 'channel_positions.npy' , 'pc_features.npy' , 'pc_feature_ind.npy' , 'similar_templates.npy' , 'spike_templates.npy' , 'spike_times.npy' , 'spike_times_sec.npy' , 'spike_times_sec_adj.npy' , 'template_features.npy' , 'template_feature_ind.npy' , 'templates.npy' , 'templates_ind.npy' , 'whitening_mat.npy' , 'whitening_mat_inv.npy' , 'spike_clusters.npy' , 'cluster_groups.csv' , 'cluster_KSLabel.tsv' ] # keys to self.files, .data are file name e.g. self.data['params'], etc. kilosort_keys = [ path . splitext ( kilosort_file )[ 0 ] for kilosort_file in kilosort_files ] def __init__ ( self , kilosort_dir ): self . _kilosort_dir = pathlib . Path ( kilosort_dir ) self . _files = {} self . _data = None self . _clusters = None params_filepath = kilosort_dir / 'params.py' if not params_filepath . exists (): raise FileNotFoundError ( f 'No Kilosort output found in: { kilosort_dir } ' ) self . _info = { 'time_created' : datetime . fromtimestamp ( params_filepath . stat () . st_ctime ), 'time_modified' : datetime . fromtimestamp ( params_filepath . stat () . st_mtime )} @property def data ( self ): if self . _data is None : self . _stat () return self . _data @property def info ( self ): return self . _info def _stat ( self ): self . _data = {} for kilosort_filename in Kilosort . kilosort_files : kilosort_filepath = self . _kilosort_dir / kilosort_filename if not kilosort_filepath . exists (): log . debug ( 'skipping {} - does not exist' . format ( kilosort_filepath )) continue base , ext = path . splitext ( kilosort_filename ) self . _files [ base ] = kilosort_filepath if kilosort_filename == 'params.py' : log . debug ( 'loading params.py {} ' . format ( kilosort_filepath )) # params.py is a 'key = val' file params = {} for line in open ( kilosort_filepath , 'r' ) . readlines (): k , v = line . strip ( ' \\n ' ) . split ( '=' ) params [ k . strip ()] = convert_to_number ( v . strip ()) log . debug ( 'params: {} ' . format ( params )) self . _data [ base ] = params if ext == '.npy' : log . debug ( 'loading npy {} ' . format ( kilosort_filepath )) d = np . load ( kilosort_filepath , mmap_mode = 'r' , allow_pickle = False , fix_imports = False ) self . _data [ base ] = ( np . reshape ( d , d . shape [ 0 ]) if d . ndim == 2 and d . shape [ 1 ] == 1 else d ) self . _data [ 'channel_map' ] = self . _data [ 'channel_map' ] . flatten () # Read the Cluster Groups for cluster_pattern , cluster_col_name in zip ([ 'cluster_groups.*' , 'cluster_KSLabel.*' ], [ 'group' , 'KSLabel' ]): try : cluster_file = next ( self . _kilosort_dir . glob ( cluster_pattern )) except StopIteration : pass else : cluster_file_suffix = cluster_file . suffix assert cluster_file_suffix in ( '.csv' , '.tsv' , '.xlsx' ) break else : raise FileNotFoundError ( 'Neither \"cluster_groups\" nor \"cluster_KSLabel\" file found!' ) if cluster_file_suffix == '.tsv' : df = pd . read_csv ( cluster_file , sep = ' \\t ' , header = 0 ) elif cluster_file_suffix == '.xlsx' : df = pd . read_excel ( cluster_file , engine = 'openpyxl' ) else : df = pd . read_csv ( cluster_file , delimiter = ' \\t ' ) self . _data [ 'cluster_groups' ] = np . array ( df [ cluster_col_name ] . values ) self . _data [ 'cluster_ids' ] = np . array ( df [ 'cluster_id' ] . values ) def get_best_channel ( self , unit ): template_idx = self . data [ 'spike_templates' ][ np . where ( self . data [ 'spike_clusters' ] == unit )[ 0 ][ 0 ]] channel_templates = self . data [ 'templates' ][ template_idx , :, :] max_channel_idx = np . abs ( channel_templates ) . max ( axis = 0 ) . argmax () max_channel = self . data [ 'channel_map' ][ max_channel_idx ] return max_channel , max_channel_idx def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ] extract_spike_depths () \u00b6 Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m Source code in element_array_ephys/readers/kilosort.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ]", "title": "kilosort.py"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort", "text": "Source code in element_array_ephys/readers/kilosort.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class Kilosort : kilosort_files = [ 'params.py' , 'amplitudes.npy' , 'channel_map.npy' , 'channel_positions.npy' , 'pc_features.npy' , 'pc_feature_ind.npy' , 'similar_templates.npy' , 'spike_templates.npy' , 'spike_times.npy' , 'spike_times_sec.npy' , 'spike_times_sec_adj.npy' , 'template_features.npy' , 'template_feature_ind.npy' , 'templates.npy' , 'templates_ind.npy' , 'whitening_mat.npy' , 'whitening_mat_inv.npy' , 'spike_clusters.npy' , 'cluster_groups.csv' , 'cluster_KSLabel.tsv' ] # keys to self.files, .data are file name e.g. self.data['params'], etc. kilosort_keys = [ path . splitext ( kilosort_file )[ 0 ] for kilosort_file in kilosort_files ] def __init__ ( self , kilosort_dir ): self . _kilosort_dir = pathlib . Path ( kilosort_dir ) self . _files = {} self . _data = None self . _clusters = None params_filepath = kilosort_dir / 'params.py' if not params_filepath . exists (): raise FileNotFoundError ( f 'No Kilosort output found in: { kilosort_dir } ' ) self . _info = { 'time_created' : datetime . fromtimestamp ( params_filepath . stat () . st_ctime ), 'time_modified' : datetime . fromtimestamp ( params_filepath . stat () . st_mtime )} @property def data ( self ): if self . _data is None : self . _stat () return self . _data @property def info ( self ): return self . _info def _stat ( self ): self . _data = {} for kilosort_filename in Kilosort . kilosort_files : kilosort_filepath = self . _kilosort_dir / kilosort_filename if not kilosort_filepath . exists (): log . debug ( 'skipping {} - does not exist' . format ( kilosort_filepath )) continue base , ext = path . splitext ( kilosort_filename ) self . _files [ base ] = kilosort_filepath if kilosort_filename == 'params.py' : log . debug ( 'loading params.py {} ' . format ( kilosort_filepath )) # params.py is a 'key = val' file params = {} for line in open ( kilosort_filepath , 'r' ) . readlines (): k , v = line . strip ( ' \\n ' ) . split ( '=' ) params [ k . strip ()] = convert_to_number ( v . strip ()) log . debug ( 'params: {} ' . format ( params )) self . _data [ base ] = params if ext == '.npy' : log . debug ( 'loading npy {} ' . format ( kilosort_filepath )) d = np . load ( kilosort_filepath , mmap_mode = 'r' , allow_pickle = False , fix_imports = False ) self . _data [ base ] = ( np . reshape ( d , d . shape [ 0 ]) if d . ndim == 2 and d . shape [ 1 ] == 1 else d ) self . _data [ 'channel_map' ] = self . _data [ 'channel_map' ] . flatten () # Read the Cluster Groups for cluster_pattern , cluster_col_name in zip ([ 'cluster_groups.*' , 'cluster_KSLabel.*' ], [ 'group' , 'KSLabel' ]): try : cluster_file = next ( self . _kilosort_dir . glob ( cluster_pattern )) except StopIteration : pass else : cluster_file_suffix = cluster_file . suffix assert cluster_file_suffix in ( '.csv' , '.tsv' , '.xlsx' ) break else : raise FileNotFoundError ( 'Neither \"cluster_groups\" nor \"cluster_KSLabel\" file found!' ) if cluster_file_suffix == '.tsv' : df = pd . read_csv ( cluster_file , sep = ' \\t ' , header = 0 ) elif cluster_file_suffix == '.xlsx' : df = pd . read_excel ( cluster_file , engine = 'openpyxl' ) else : df = pd . read_csv ( cluster_file , delimiter = ' \\t ' ) self . _data [ 'cluster_groups' ] = np . array ( df [ cluster_col_name ] . values ) self . _data [ 'cluster_ids' ] = np . array ( df [ 'cluster_id' ] . values ) def get_best_channel ( self , unit ): template_idx = self . data [ 'spike_templates' ][ np . where ( self . data [ 'spike_clusters' ] == unit )[ 0 ][ 0 ]] channel_templates = self . data [ 'templates' ][ template_idx , :, :] max_channel_idx = np . abs ( channel_templates ) . max ( axis = 0 ) . argmax () max_channel = self . data [ 'channel_map' ][ max_channel_idx ] return max_channel , max_channel_idx def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ]", "title": "Kilosort"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.extract_spike_depths", "text": "Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m Source code in element_array_ephys/readers/kilosort.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ]", "title": "extract_spike_depths()"}, {"location": "api/element_array_ephys/readers/openephys/", "text": "OpenEphys \u00b6 Source code in element_array_ephys/readers/openephys.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class OpenEphys : def __init__ ( self , experiment_dir ): self . session_dir = pathlib . Path ( experiment_dir ) openephys_file = pyopenephys . File ( self . session_dir . parent ) # this is on the Record Node level # extract the \"recordings\" for this session self . experiment = next ( experiment for experiment in openephys_file . experiments if pathlib . Path ( experiment . absolute_foldername ) == self . session_dir ) self . recording_time = self . experiment . datetime # extract probe data self . probes = self . load_probe_data () def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} for processor in self . experiment . settings [ 'SIGNALCHAIN' ][ 'PROCESSOR' ]: if processor [ '@pluginName' ] in ( 'Neuropix-PXI' , 'Neuropix-3a' ): if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): probe = Probe ( processor ) probes [ probe . probe_SN ] = probe else : for probe_index in range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])): probe = Probe ( processor , probe_index ) probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime ) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' meta = getattr ( probe , continuous_type + '_meta' ) if not meta : meta . update ( ** continuous_info , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes load_probe_data () \u00b6 Loop through all Open Ephys \"processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe Source code in element_array_ephys/readers/openephys.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} for processor in self . experiment . settings [ 'SIGNALCHAIN' ][ 'PROCESSOR' ]: if processor [ '@pluginName' ] in ( 'Neuropix-PXI' , 'Neuropix-3a' ): if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): probe = Probe ( processor ) probes [ probe . probe_SN ] = probe else : for probe_index in range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])): probe = Probe ( processor , probe_index ) probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime ) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' meta = getattr ( probe , continuous_type + '_meta' ) if not meta : meta . update ( ** continuous_info , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes Probe \u00b6 Source code in element_array_ephys/readers/openephys.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class Probe : def __init__ ( self , processor , probe_index = 0 ): self . processor_id = int ( processor [ '@NodeId' ]) if processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]: self . probe_info = processor [ 'EDITOR' ][ 'PROBE' ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = { \"Neuropix-PXI\" : \"neuropixels 1.0 - 3B\" , \"Neuropix-3a\" : \"neuropixels 1.0 - 3A\" }[ processor [ '@pluginName' ]] else : self . probe_info = processor [ 'EDITOR' ][ 'NP_PROBE' ][ probe_index ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = self . probe_info [ '@probe_name' ] self . ap_meta = {} self . lfp_meta = {} self . ap_analog_signals = [] self . lfp_analog_signals = [] self . recording_info = { 'recording_count' : 0 , 'recording_datetimes' : [], 'recording_durations' : [], 'recording_files' : []} self . _ap_timeseries = None self . _ap_timestamps = None self . _lfp_timeseries = None self . _lfp_timestamps = None @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries @property def ap_timestamps ( self ): if self . _ap_timestamps is None : self . _ap_timestamps = np . hstack ([ s . times for s in self . ap_analog_signals ]) return self . _ap_timestamps @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries @property def lfp_timestamps ( self ): if self . _lfp_timestamps is None : self . _lfp_timestamps = np . hstack ([ s . times for s in self . lfp_analog_signals ]) return self . _lfp_timestamps def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) ap_timeseries () property \u00b6 AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 141 142 143 144 145 146 147 148 149 150 @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries extract_spike_waveforms ( spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )) \u00b6 :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) Source code in element_array_ephys/readers/openephys.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) lfp_timeseries () property \u00b6 LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 158 159 160 161 162 163 164 165 166 167 @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries", "title": "openephys.py"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys", "text": "Source code in element_array_ephys/readers/openephys.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class OpenEphys : def __init__ ( self , experiment_dir ): self . session_dir = pathlib . Path ( experiment_dir ) openephys_file = pyopenephys . File ( self . session_dir . parent ) # this is on the Record Node level # extract the \"recordings\" for this session self . experiment = next ( experiment for experiment in openephys_file . experiments if pathlib . Path ( experiment . absolute_foldername ) == self . session_dir ) self . recording_time = self . experiment . datetime # extract probe data self . probes = self . load_probe_data () def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} for processor in self . experiment . settings [ 'SIGNALCHAIN' ][ 'PROCESSOR' ]: if processor [ '@pluginName' ] in ( 'Neuropix-PXI' , 'Neuropix-3a' ): if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): probe = Probe ( processor ) probes [ probe . probe_SN ] = probe else : for probe_index in range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])): probe = Probe ( processor , probe_index ) probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime ) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' meta = getattr ( probe , continuous_type + '_meta' ) if not meta : meta . update ( ** continuous_info , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes", "title": "OpenEphys"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys.load_probe_data", "text": "Loop through all Open Ephys \"processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe Source code in element_array_ephys/readers/openephys.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} for processor in self . experiment . settings [ 'SIGNALCHAIN' ][ 'PROCESSOR' ]: if processor [ '@pluginName' ] in ( 'Neuropix-PXI' , 'Neuropix-3a' ): if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): probe = Probe ( processor ) probes [ probe . probe_SN ] = probe else : for probe_index in range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])): probe = Probe ( processor , probe_index ) probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime ) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' meta = getattr ( probe , continuous_type + '_meta' ) if not meta : meta . update ( ** continuous_info , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes", "title": "load_probe_data()"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe", "text": "Source code in element_array_ephys/readers/openephys.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class Probe : def __init__ ( self , processor , probe_index = 0 ): self . processor_id = int ( processor [ '@NodeId' ]) if processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]: self . probe_info = processor [ 'EDITOR' ][ 'PROBE' ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = { \"Neuropix-PXI\" : \"neuropixels 1.0 - 3B\" , \"Neuropix-3a\" : \"neuropixels 1.0 - 3A\" }[ processor [ '@pluginName' ]] else : self . probe_info = processor [ 'EDITOR' ][ 'NP_PROBE' ][ probe_index ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = self . probe_info [ '@probe_name' ] self . ap_meta = {} self . lfp_meta = {} self . ap_analog_signals = [] self . lfp_analog_signals = [] self . recording_info = { 'recording_count' : 0 , 'recording_datetimes' : [], 'recording_durations' : [], 'recording_files' : []} self . _ap_timeseries = None self . _ap_timestamps = None self . _lfp_timeseries = None self . _lfp_timestamps = None @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries @property def ap_timestamps ( self ): if self . _ap_timestamps is None : self . _ap_timestamps = np . hstack ([ s . times for s in self . ap_analog_signals ]) return self . _ap_timestamps @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries @property def lfp_timestamps ( self ): if self . _lfp_timestamps is None : self . _lfp_timestamps = np . hstack ([ s . times for s in self . lfp_analog_signals ]) return self . _lfp_timestamps def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan )", "title": "Probe"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.ap_timeseries", "text": "AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 141 142 143 144 145 146 147 148 149 150 @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries", "title": "ap_timeseries()"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.extract_spike_waveforms", "text": ":param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) Source code in element_array_ephys/readers/openephys.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan )", "title": "extract_spike_waveforms()"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.lfp_timeseries", "text": "LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 158 159 160 161 162 163 164 165 166 167 @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries", "title": "lfp_timeseries()"}, {"location": "api/element_array_ephys/readers/spikeglx/", "text": "SpikeGLX \u00b6 Source code in element_array_ephys/readers/spikeglx.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 class SpikeGLX : def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' ) @property def apmeta ( self ): if self . _apmeta is None : self . _apmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.ap.meta' )) return self . _apmeta @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries @property def lfmeta ( self ): if self . _lfmeta is None : self . _lfmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.lf.meta' )) return self . _lfmeta @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well def _read_bin ( self , fname ): nchan = self . apmeta . meta [ 'nSavedChans' ] dtype = np . dtype (( np . int16 , nchan )) return np . memmap ( fname , dtype , 'r' ) def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) __init__ ( root_dir ) \u00b6 create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. Source code in element_array_ephys/readers/spikeglx.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' ) ap_timeseries () property \u00b6 AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') Source code in element_array_ephys/readers/spikeglx.py 53 54 55 56 57 58 59 60 61 62 @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries extract_spike_waveforms ( spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )) \u00b6 :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) Source code in element_array_ephys/readers/spikeglx.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) get_channel_bit_volts ( band = 'ap' ) \u00b6 Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain Source code in element_array_ephys/readers/spikeglx.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well lf_timeseries () property \u00b6 LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') Source code in element_array_ephys/readers/spikeglx.py 70 71 72 73 74 75 76 77 78 79 @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries SpikeGLXMeta \u00b6 Source code in element_array_ephys/readers/spikeglx.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 class SpikeGLXMeta : def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )] @staticmethod def _parse_chanmap ( raw ): ''' https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#channel-map Parse channel map header structure. Converts: '(x,y,z)(c0,x:y)...(cI,x:y),(sy0;x:y)' e.g: '(384,384,1)(AP0;0:0)...(AP383;383:383)(SY0;768:768)' into dict of form: {'shape': [x,y,z], 'c0': [x,y], ... } ''' res = {} for u in ( i . rstrip ( ')' ) . split ( ';' ) for i in raw . split ( '(' ) if i != '' ): if ( len ( u )) == 1 : res [ 'shape' ] = u [ 0 ] . split ( ',' ) else : res [ u [ 0 ]] = u [ 1 ] . split ( ':' ) return res @staticmethod def _parse_shankmap ( raw ): \"\"\" The shankmap contains details on the shank info for each electrode sites of the sites being recorded only https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#shank-map Parse shank map header structure. Converts: '(x,y,z)(a:b:c:d)...(a:b:c:d)' e.g: '(1,2,480)(0:0:192:1)...(0:1:191:1)' into dict of form: {'shape': [x,y,z], 'data': [[a,b,c,d],...]} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ':' )]) return res @staticmethod def _parse_imrotbl ( raw ): \"\"\" The imro table contains info for all electrode sites (no sync) for a particular electrode configuration (all 384 sites) Note: not all of these 384 sites are necessarily recorded https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#imro-per-channel-settings Parse imro tbl structure. Converts: '(X,Y,Z)(A B C D E)...(A B C D E)' e.g.: '(641251209,3,384)(0 1 0 500 250)...(383 0 0 500 250)' into dict of form: {'shape': (x,y,z), 'data': []} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ' ' )]) return res def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels __init__ ( meta_filepath ) \u00b6 Some good processing references https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m Source code in element_array_ephys/readers/spikeglx.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )] get_original_chans () \u00b6 Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function Source code in element_array_ephys/readers/spikeglx.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels get_recording_channels_indices ( exclude_sync = False ) \u00b6 The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table Source code in element_array_ephys/readers/spikeglx.py 284 285 286 287 288 289 290 291 292 293 294 295 def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind", "title": "spikeglx.py"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX", "text": "Source code in element_array_ephys/readers/spikeglx.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 class SpikeGLX : def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' ) @property def apmeta ( self ): if self . _apmeta is None : self . _apmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.ap.meta' )) return self . _apmeta @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries @property def lfmeta ( self ): if self . _lfmeta is None : self . _lfmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.lf.meta' )) return self . _lfmeta @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well def _read_bin ( self , fname ): nchan = self . apmeta . meta [ 'nSavedChans' ] dtype = np . dtype (( np . int16 , nchan )) return np . memmap ( fname , dtype , 'r' ) def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan )", "title": "SpikeGLX"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.__init__", "text": "create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. Source code in element_array_ephys/readers/spikeglx.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' )", "title": "__init__()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.ap_timeseries", "text": "AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') Source code in element_array_ephys/readers/spikeglx.py 53 54 55 56 57 58 59 60 61 62 @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries", "title": "ap_timeseries()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.extract_spike_waveforms", "text": ":param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) Source code in element_array_ephys/readers/spikeglx.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan )", "title": "extract_spike_waveforms()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.get_channel_bit_volts", "text": "Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain Source code in element_array_ephys/readers/spikeglx.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well", "title": "get_channel_bit_volts()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.lf_timeseries", "text": "LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') Source code in element_array_ephys/readers/spikeglx.py 70 71 72 73 74 75 76 77 78 79 @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries", "title": "lf_timeseries()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta", "text": "Source code in element_array_ephys/readers/spikeglx.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 class SpikeGLXMeta : def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )] @staticmethod def _parse_chanmap ( raw ): ''' https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#channel-map Parse channel map header structure. Converts: '(x,y,z)(c0,x:y)...(cI,x:y),(sy0;x:y)' e.g: '(384,384,1)(AP0;0:0)...(AP383;383:383)(SY0;768:768)' into dict of form: {'shape': [x,y,z], 'c0': [x,y], ... } ''' res = {} for u in ( i . rstrip ( ')' ) . split ( ';' ) for i in raw . split ( '(' ) if i != '' ): if ( len ( u )) == 1 : res [ 'shape' ] = u [ 0 ] . split ( ',' ) else : res [ u [ 0 ]] = u [ 1 ] . split ( ':' ) return res @staticmethod def _parse_shankmap ( raw ): \"\"\" The shankmap contains details on the shank info for each electrode sites of the sites being recorded only https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#shank-map Parse shank map header structure. Converts: '(x,y,z)(a:b:c:d)...(a:b:c:d)' e.g: '(1,2,480)(0:0:192:1)...(0:1:191:1)' into dict of form: {'shape': [x,y,z], 'data': [[a,b,c,d],...]} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ':' )]) return res @staticmethod def _parse_imrotbl ( raw ): \"\"\" The imro table contains info for all electrode sites (no sync) for a particular electrode configuration (all 384 sites) Note: not all of these 384 sites are necessarily recorded https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#imro-per-channel-settings Parse imro tbl structure. Converts: '(X,Y,Z)(A B C D E)...(A B C D E)' e.g.: '(641251209,3,384)(0 1 0 500 250)...(383 0 0 500 250)' into dict of form: {'shape': (x,y,z), 'data': []} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ' ' )]) return res def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels", "title": "SpikeGLXMeta"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.__init__", "text": "Some good processing references https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m Source code in element_array_ephys/readers/spikeglx.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )]", "title": "__init__()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_original_chans", "text": "Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function Source code in element_array_ephys/readers/spikeglx.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels", "title": "get_original_chans()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_recording_channels_indices", "text": "The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table Source code in element_array_ephys/readers/spikeglx.py 284 285 286 287 288 289 290 291 292 293 294 295 def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind", "title": "get_recording_channels_indices()"}, {"location": "api/element_array_ephys/readers/utils/", "text": "", "title": "utils.py"}]}